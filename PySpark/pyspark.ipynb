{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nisse</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ann</td>\n",
       "      <td>47</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Name  Age  Experience\n",
       "0  Nisse   15           7\n",
       "1    Ann   47           4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName(\"test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Surf.mynet:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x22abf9ae588>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv(\"test.csv\", header=True, inferSchema=True)\n",
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+\n",
      "| Name|Age|Experience|\n",
      "+-----+---+----------+\n",
      "|Nisse| 15|         7|\n",
      "|  Ann| 47|         4|\n",
      "+-----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "| Name|Experience|\n",
      "+-----+----------+\n",
      "|Nisse|         7|\n",
      "|  Ann|         4|\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select([\"Name\", \"Experience\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'), ('Age', 'int'), ('Experience', 'int')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------------------+------------------+\n",
      "|summary| Name|               Age|        Experience|\n",
      "+-------+-----+------------------+------------------+\n",
      "|  count|    2|                 2|                 2|\n",
      "|   mean| null|              31.0|               5.5|\n",
      "| stddev| null|22.627416997969522|2.1213203435596424|\n",
      "|    min|  Ann|                15|                 4|\n",
      "|    max|Nisse|                47|                 7|\n",
      "+-------+-----+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------------------------+\n",
      "| Name|Age|Experience|Experience after 2 years|\n",
      "+-----+---+----------+------------------------+\n",
      "|Nisse| 15|         7|                       9|\n",
      "|  Ann| 47|         4|                       6|\n",
      "+-----+---+----------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = df_pyspark.withColumn(\"Experience after 2 years\", df_pyspark[\"Experience\"]+2)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+\n",
      "| Name|Age|Experience|\n",
      "+-----+---+----------+\n",
      "|Nisse| 15|         7|\n",
      "|  Ann| 47|         4|\n",
      "+-----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark=df_pyspark.drop(\"Experience after 2 years\")\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+\n",
      "|New Name|Age|Experience|\n",
      "+--------+---+----------+\n",
      "|   Nisse| 15|         7|\n",
      "|     Ann| 47|         4|\n",
      "+--------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.withColumnRenamed(\"Name\", \"New Name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName(\"practise\").getOrCreate()\n",
    "\n",
    "df_pyspark = spark.read.csv(\"test2.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+------+\n",
      "| Name| Age|Experience|Salery|\n",
      "+-----+----+----------+------+\n",
      "|Nisse|  15|         7|    33|\n",
      "|  Ann|  47|         4|    42|\n",
      "|Bengt|null|      null|  null|\n",
      "| null|  22|        25|  null|\n",
      "+-----+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------+\n",
      "| Age|Experience|Salery|\n",
      "+----+----------+------+\n",
      "|  15|         7|    33|\n",
      "|  47|         4|    42|\n",
      "|null|      null|  null|\n",
      "|  22|        25|  null|\n",
      "+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.drop(\"Name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+\n",
      "| Name|Age|Experience|Salery|\n",
      "+-----+---+----------+------+\n",
      "|Nisse| 15|         7|    33|\n",
      "|  Ann| 47|         4|    42|\n",
      "+-----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how=\"any\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+\n",
      "| Name|Age|Experience|Salery|\n",
      "+-----+---+----------+------+\n",
      "|Nisse| 15|         7|    33|\n",
      "|  Ann| 47|         4|    42|\n",
      "| null| 22|        25|  null|\n",
      "+-----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how=\"any\", thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+\n",
      "| Name|Age|Experience|Salery|\n",
      "+-----+---+----------+------+\n",
      "|Nisse| 15|         7|    33|\n",
      "|  Ann| 47|         4|    42|\n",
      "+-----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how=\"any\", thresh=3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+\n",
      "| Name|Age|Experience|Salery|\n",
      "+-----+---+----------+------+\n",
      "|Nisse| 15|         7|    33|\n",
      "|  Ann| 47|         4|    42|\n",
      "| null| 22|        25|  null|\n",
      "+-----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how=\"any\", subset=[\"Age\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   Name| Age|Experience|Salery|\n",
      "+-------+----+----------+------+\n",
      "|  Nisse|  15|         7|    33|\n",
      "|    Ann|  47|         4|    42|\n",
      "|  Bengt|null|      null|  null|\n",
      "|Missing|  22|        25|  null|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_pyspark.na.fill(value=\"Missing\",subset=\"Name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+\n",
      "| Name|Age|Experience|Salery|\n",
      "+-----+---+----------+------+\n",
      "|Nisse| 15|         7|    33|\n",
      "|  Ann| 47|         4|    42|\n",
      "|Bengt|  0|         0|  null|\n",
      "| null| 22|        25|  null|\n",
      "+-----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.fill(value=0,subset=[\"Age\", \"Experience\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(inputCols=[\"Age\", \"Experience\"], outputCols=[\"Age_filled\", \"Experience_filled\"]).setStrategy(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+------+----------+-----------------+\n",
      "| Name| Age|Experience|Salery|Age_filled|Experience_filled|\n",
      "+-----+----+----------+------+----------+-----------------+\n",
      "|Nisse|  15|         7|    33|        15|                7|\n",
      "|  Ann|  47|         4|    42|        47|                4|\n",
      "|Bengt|null|      null|  null|        28|               12|\n",
      "| null|  22|        25|  null|        22|               25|\n",
      "+-----+----+----------+------+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+------+\n",
      "|Name|Age|Experience|Salery|\n",
      "+----+---+----------+------+\n",
      "|null| 22|        25|  null|\n",
      "+----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter((df_pyspark[\"Age\"]<47) & (df_pyspark[\"Experience\"]>10)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| Name|\n",
      "+-----+\n",
      "|Nisse|\n",
      "| null|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter(df_pyspark[\"Age\"]<47).select(\"Name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Name|\n",
      "+----+\n",
      "| Ann|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter(~(df_pyspark[\"Age\"]<47)).select(\"Name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------+\n",
      "| Name|Department|Salery|\n",
      "+-----+----------+------+\n",
      "|Nisse|      dep1|  71.0|\n",
      "|  Ann|      dep1|  41.0|\n",
      "|Bengt|      dep2|  55.0|\n",
      "|Sunny|      dep2|  22.0|\n",
      "|Sunny|      dep3|  11.0|\n",
      "|Bengt|      dep3|   2.0|\n",
      "+-----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv(\"test3.csv\", header=True, inferSchema=True)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "| Name|sum(Salery)|\n",
      "+-----+-----------+\n",
      "|Bengt|       57.0|\n",
      "|Sunny|       33.0|\n",
      "|  Ann|       41.0|\n",
      "|Nisse|       71.0|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_pyspark.groupBy(\"Name\").sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|Department|sum(Salery)|\n",
      "+----------+-----------+\n",
      "|      dep3|       13.0|\n",
      "|      dep2|       77.0|\n",
      "|      dep1|      112.0|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy(\"Department\").sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|Department|avg(Salery)|\n",
      "+----------+-----------+\n",
      "|      dep3|        6.5|\n",
      "|      dep2|       38.5|\n",
      "|      dep1|       56.0|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy(\"Department\").mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|Department|count|\n",
      "+----------+-----+\n",
      "|      dep3|    2|\n",
      "|      dep2|    2|\n",
      "|      dep1|    2|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy(\"Department\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "| Name|count|\n",
      "+-----+-----+\n",
      "|Bengt|    2|\n",
      "|Sunny|    2|\n",
      "|  Ann|    1|\n",
      "|Nisse|    1|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy(\"Name\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|       avg(Salery)|\n",
      "+------------------+\n",
      "|33.666666666666664|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.agg({\"Salery\": \"mean\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "| Name|max(Salery)|\n",
      "+-----+-----------+\n",
      "|Bengt|       55.0|\n",
      "|Sunny|       22.0|\n",
      "|  Ann|       41.0|\n",
      "|Nisse|       71.0|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy(\"Name\").max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "| Name|avg(Salery)|\n",
      "+-----+-----------+\n",
      "|Bengt|       28.5|\n",
      "|Sunny|       16.5|\n",
      "|  Ann|       41.0|\n",
      "|Nisse|       71.0|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy(\"Name\").avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salery: integer (nullable = true)\n",
      "\n",
      "+-----+---+----------+------+-------------------+\n",
      "| Name|Age|Experience|Salery|Independet features|\n",
      "+-----+---+----------+------+-------------------+\n",
      "|Nisse| 15|         7|    33|         [15.0,7.0]|\n",
      "|  Ann| 47|         4|    42|         [47.0,4.0]|\n",
      "|Bengt| 33|        44|    55|        [33.0,44.0]|\n",
      "|Julia| 22|        25|    44|        [22.0,25.0]|\n",
      "+-----+---+----------+------+-------------------+\n",
      "\n",
      "+-------------------+------+\n",
      "|Independet features|Salery|\n",
      "+-------------------+------+\n",
      "|         [15.0,7.0]|    33|\n",
      "|         [47.0,4.0]|    42|\n",
      "|        [33.0,44.0]|    55|\n",
      "|        [22.0,25.0]|    44|\n",
      "+-------------------+------+\n",
      "\n",
      "+-------------------+------+------------------+\n",
      "|Independet features|Salery|        prediction|\n",
      "+-------------------+------+------------------+\n",
      "|        [22.0,25.0]|    44|  38.9219330855019|\n",
      "|        [33.0,44.0]|    55|46.260223048327205|\n",
      "+-------------------+------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.6887680124481199"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "spark = SparkSession.builder.appName(\"TestML\").getOrCreate()\n",
    "\n",
    "data = spark.read.csv(\"test4.csv\", header=True, inferSchema=True)\n",
    "\n",
    "#data.printSchema()\n",
    "#data.columns\n",
    "\n",
    "\n",
    "featureassembler = VectorAssembler(inputCols = [\"Age\", \"Experience\"], outputCol=\"Independet features\")\n",
    "output=featureassembler.transform(data)\n",
    "\n",
    "#output.show()\n",
    "#output.columns\n",
    "\n",
    "finalized_data = output.select(\"Independet features\", \"Salery\")\n",
    "\n",
    "#finalized_data.show()\n",
    "\n",
    "train_data, validate_data = finalized_data.randomSplit([0.6, 0.4])\n",
    "regressor = LinearRegression(featuresCol=\"Independet features\", labelCol=\"Salery\")\n",
    "regressor = regressor.fit(train_data)\n",
    "\n",
    "regressor.coefficients\n",
    "regressor.intercept\n",
    "\n",
    "pred_result = regressor.evaluate(validate_data)\n",
    "\n",
    "\n",
    "pred_result.predictions.show()\n",
    "pred_result.meanAbsoluteError, pred_result.meanSquaredError\n",
    "pred_result.r2"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2581d027f1d7e6828ac62a770177d952985a76d9de499a5c3eb533096347499f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
